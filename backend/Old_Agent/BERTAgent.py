import os
import torch
import logging
import numpy as np
import pandas as pd
from typing import List, Dict
from fastapi import HTTPException
from pydantic import BaseModel, validator
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime
import json

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("bert_agent.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class RAGRequest(BaseModel):
    query: str

    @validator('query')
    def validate_query(cls, v):
        if len(v) > 100:
            raise ValueError('æŸ¥è©¢é•·åº¦ä¸å¯è¶…é100å­—')
        return v


class PromptTemplate(BaseModel):
    """
    å°ˆé–€ç‚ºå¤§å­¸å­¸ç¾¤ RAG ç³»çµ±è¨­è¨ˆçš„æç¤ºæ¨¡æ¿ç®¡ç†é¡åˆ¥
    """
    @staticmethod
    def load_predefined_prompts(file_path: str = 'predefined_prompts.json') -> Dict[str, str]:
        """
        è¼‰å…¥é å®šç¾©çš„æç¤ºæ¨¡æ¿
        """
        try:
            if not os.path.exists(file_path):
                # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œå»ºç«‹ä¸€å€‹é è¨­çš„æç¤ºæ¨¡æ¿
                default_prompts = {
                    "general_inquiry": "æ ¹æ“šæ‚¨çš„æŸ¥è©¢ã€Œ{query}ã€ï¼Œæˆ‘ç‚ºæ‚¨æ‰¾åˆ°äº†ä»¥ä¸‹æœ€ç›¸é—œçš„å­¸ç¾¤è³‡è¨Šã€‚è«‹åƒè€ƒæ¨è–¦ï¼Œé€™äº›å­¸ç¾¤å¯èƒ½éå¸¸é©åˆæ‚¨çš„èˆˆè¶£å’Œæœªä¾†ç™¼å±•ã€‚",
                    "career_guidance": "æ‚¨çš„æŸ¥è©¢ã€Œ{query}ã€é¡¯ç¤ºå‡ºå°ç‰¹å®šå°ˆæ¥­é ˜åŸŸçš„èˆˆè¶£ã€‚ä»¥ä¸‹å­¸ç¾¤æ¨è–¦ä¸åƒ…æä¾›å­¸è¡“çŸ¥è­˜ï¼Œæ›´èƒ½å¹«åŠ©æ‚¨è¦åŠƒæœªä¾†è·æ¶¯ç™¼å±•ã€‚",
                    "academic_exploration": "å°æ–¼ã€Œ{query}ã€çš„æ¢ç´¢ï¼Œæˆ‘å€‘ç‚ºæ‚¨ç²¾é¸äº†æœ€åŒ¹é…çš„å­¸ç¾¤ã€‚é€™äº›æ¨è–¦å°‡å”åŠ©æ‚¨æ›´æ·±å…¥äº†è§£ä¸åŒå­¸è¡“é ˜åŸŸçš„ç‰¹è‰²å’Œå¯èƒ½æ€§ã€‚"
                }

                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(default_prompts, f, ensure_ascii=False, indent=4)

            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logging.error(f"è¼‰å…¥æç¤ºæ¨¡æ¿æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {}

    @staticmethod
    def generate_enhanced_context(query: str, top_groups: List[Dict], prompt_type: str = "general_inquiry") -> str:
        """
        æ ¹æ“šä¸åŒçš„æç¤ºé¡å‹ï¼Œç”Ÿæˆæ›´è±å¯Œã€æ›´æœ‰é‡å°æ€§çš„ä¸Šä¸‹æ–‡
        """
        # è¼‰å…¥æç¤ºæ¨¡æ¿
        prompts = PromptTemplate.load_predefined_prompts()
        base_prompt = prompts.get(prompt_type, prompts.get("general_inquiry"))

        # æ ¹æ“šæç¤ºé¡å‹å®¢è£½åŒ–è¼¸å‡º
        context_intro = base_prompt.format(query=query) + "\n\n"
        context_intro += "ğŸ” æ™ºèƒ½æ¨è–¦å ±å‘Šï¼šç²¾æº–åŒ¹é…æ‚¨çš„å­¸è¡“èˆˆè¶£\n\n"

        context_groups = ""
        for idx, group in enumerate(top_groups, 1):
            context_groups += f"ğŸ“ æ¨è–¦å­¸ç¾¤ {idx}ï¼š{group['group_name']}\n"
            context_groups += f"   â€¢ å­¸ç¾¤ç‰¹è‰²ï¼š{group['introduction']}\n"
            context_groups += f"   â€¢ èª²ç¨‹äº®é»ï¼š{group['learning_content']}\n"
            context_groups += f"   â€¢ ç›¸é—œé ˜åŸŸï¼š{group.get('related_groups', 'å°šæœªæä¾›ç›¸é—œé ˜åŸŸè³‡è¨Š')}\n"
            context_groups += f"   â€¢ æ›´å¤šè³‡è¨Šï¼š{group['link']}\n\n"

        context_outro = "ğŸ’¡ å°ˆæ¥­å»ºè­°ï¼š\n"
        context_outro += "   â€¢ æ·±å…¥ç ”ç©¶æ¯å€‹æ¨è–¦å­¸ç¾¤çš„å®Œæ•´ä»‹ç´¹\n"
        context_outro += "   â€¢ åƒè€ƒå­¸ç¾¤ç‰¹è‰²ï¼Œè©•ä¼°æ˜¯å¦ç¬¦åˆå€‹äººèˆˆè¶£\n"
        context_outro += "   â€¢ ä¸»å‹•è«®è©¢å­¸æ ¡è¼”å°è€å¸«ï¼Œç²å¾—æ›´ç²¾æº–çš„é¸èª²å»ºè­°\n"

        return context_intro + context_groups + context_outro

    @staticmethod
    def suggest_prompt_type(query: str) -> str:
        """
        æ ¹æ“šæŸ¥è©¢å…§å®¹æ™ºèƒ½æ¨è–¦æç¤ºæ¨¡æ¿é¡å‹
        """
        # å®šç¾©é—œéµå­—æ˜ å°„
        keyword_mapping = {
            "career": ["è·æ¥­", "å·¥ä½œ", "æœªä¾†", "å°±æ¥­", "è·æ¶¯"],
            "academic": ["å­¸è¡“", "ç ”ç©¶", "é ˜åŸŸ", "å°ˆæ¥­", "å­¸å•"]
        }

        # è½‰æ›æŸ¥è©¢ç‚ºå°å¯«
        query_lower = query.lower()

        # æª¢æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šé—œéµå­—
        for prompt_type, keywords in keyword_mapping.items():
            if any(keyword in query_lower for keyword in keywords):
                return f"{prompt_type}_guidance"

        return "general_inquiry"


class CollegeRAG:
    def __init__(self, csv_path='college_details_detailed.csv'):
        """
        åˆå§‹åŒ– RAG ç³»çµ±ï¼Œè¼‰å…¥å¤§å­¸å­¸ç¾¤è³‡æ–™ä¸¦å»ºç«‹åµŒå…¥æ¨¡å‹ï¼Œå¢åŠ éŒ¯èª¤è™•ç†
        """
        logger.info("æ­£åœ¨åˆå§‹åŒ–CollegeRAGç³»çµ±")

        try:
            # ç¢ºèªè³‡æ–™æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å‰‡åŸ·è¡Œçˆ¬èŸ²
            if not os.path.exists(csv_path):
                logger.warning("æœªæ‰¾åˆ°å¤§å­¸å­¸ç¾¤è³‡æ–™ï¼Œæ­£åœ¨åŸ·è¡Œçˆ¬èŸ²...")
                self._scrape_college_data()

            # è¼‰å…¥è³‡æ–™
            self.df = pd.read_csv(csv_path, encoding='utf-8-sig')
            logger.info(f"æˆåŠŸè¼‰å…¥ {len(self.df)} ç­†å¤§å­¸å­¸ç¾¤è³‡æ–™")

            # åˆå§‹åŒ–æ¨¡å‹
            self.device = torch.device(
                "cuda" if torch.cuda.is_available() else "cpu")
            logger.info(f"ä½¿ç”¨è¨­å‚™: {self.device}")

            self.tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
            self.model = AutoModel.from_pretrained(
                "bert-base-chinese").to(self.device)

            # é è¨ˆç®—åµŒå…¥
            self.embeddings = self._compute_embeddings()

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–CollegeRAGç³»çµ±æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise

    def _scrape_college_data(self):
        """åŸ·è¡Œçˆ¬èŸ²æ“ä½œï¼Œè¨˜éŒ„æ—¥èªŒ"""
        try:
            from college import scrape_all_college_details
            scrape_all_college_details()
            logger.info("æˆåŠŸåŸ·è¡Œå¤§å­¸å­¸ç¾¤è³‡æ–™çˆ¬èŸ²")
        except Exception as e:
            logger.error(f"çˆ¬èŸ²ä½œæ¥­å¤±æ•—: {str(e)}")
            raise

    def _mean_pooling(self, model_output, attention_mask):
        """å‡å€¼æ± åŒ–ç”Ÿæˆå¥å­åµŒå…¥"""
        token_embeddings = model_output.last_hidden_state
        input_mask_expanded = attention_mask.unsqueeze(
            -1).expand(token_embeddings.size()).float()
        sum_embeddings = torch.sum(
            token_embeddings * input_mask_expanded, 1)
        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
        return sum_embeddings / sum_mask

    def _compute_embeddings(self, batch_size: int = 16) -> np.ndarray:
        """è¨ˆç®—æ‰€æœ‰å­¸ç¾¤æè¿°çš„å‘é‡åµŒå…¥"""
        logger.info("é–‹å§‹è¨ˆç®—å­¸ç¾¤åµŒå…¥å‘é‡")

        # çµ„åˆæœç´¢æ–‡æœ¬
        search_texts = self.df.apply(
            lambda row: f"{row.get('group_name', '')} {row.get('introduction', '')} {row.get('learning_content', '')} {row.get('related_groups', '')} {row.get('link', '')}",
            axis=1
        )

        embeddings = []

        for i in range(0, len(search_texts), batch_size):
            batch_texts = search_texts[i:i+batch_size].tolist()
            inputs = self.tokenizer(
                batch_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)

            batch_embeddings = self._mean_pooling(
                outputs, inputs['attention_mask'])
            embeddings.extend(batch_embeddings.cpu().numpy())

        logger.info(f"å®Œæˆ {len(embeddings)} å€‹å­¸ç¾¤çš„åµŒå…¥å‘é‡è¨ˆç®—")
        return np.array(embeddings)

    def retrieve_top_k_groups(self, query: str, k: int = 3) -> List[Dict]:
        """æ ¹æ“šæŸ¥è©¢è¿”å›æœ€ç›¸é—œçš„ k å€‹å­¸ç¾¤ï¼Œå¢åŠ æ—¥èªŒå’ŒéŒ¯èª¤è™•ç†"""
        start_time = datetime.now()
        logger.info(f"é–‹å§‹æª¢ç´¢æŸ¥è©¢: {query}")

        try:
            inputs = self.tokenizer(query, return_tensors="pt", padding=True,
                                    truncation=True, max_length=512).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)

            query_embedding = self._mean_pooling(
                outputs, inputs['attention_mask']).cpu().numpy().squeeze()

            similarities = cosine_similarity(
                [query_embedding], self.embeddings)[0]
            top_indices = similarities.argsort()[::-1][:k]

            results = [
                {
                    "group_name": self.df.iloc[idx]['group_name'],
                    "introduction": self.df.iloc[idx]['introduction'],
                    "learning_content": self.df.iloc[idx]['learning_content'],
                    "related_groups": self.df.iloc[idx].get('related_groups', ''),
                    "link": self.df.iloc[idx]['link'],
                    "similarity_score": similarities[idx]
                }
                for idx in top_indices
            ]

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            logger.info(f"æª¢ç´¢å®Œæˆï¼Œè€—æ™‚ {duration} ç§’ï¼Œæ‰¾åˆ° {len(results)} å€‹ç›¸é—œå­¸ç¾¤")

            self._log_retrieval(query, results)
            return results

        except Exception as e:
            logger.error(f"å­¸ç¾¤æª¢ç´¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise

    def _log_retrieval(self, query: str, results: List[Dict]):
        """è¨˜éŒ„æª¢ç´¢æ—¥èªŒåˆ°JSONLæ–‡ä»¶"""
        try:
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "query": query,
                "results": [r["group_name"] for r in results]
            }

            with open("retrieval_log.jsonl", "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.error(f"è¨˜éŒ„æª¢ç´¢æ—¥èªŒæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")


class BertRAGAgent:
    def __init__(self):
        logger.info("åˆå§‹åŒ–BertRAGAgent")
        self.rag_system = CollegeRAG()

    async def college_rag_search(self, request: RAGRequest):
        """å®¢è£½åŒ–çš„ç¹é«”ä¸­æ–‡å­¸ç¾¤æœå°‹æœå‹™ï¼Œå¢åŠ éŒ¯èª¤è™•ç†å’Œæ—¥èªŒ"""
        start_time = datetime.now()
        logger.info(f"æ”¶åˆ°å­¸ç¾¤æœå°‹è«‹æ±‚: {request.query}")

        try:
            # å–å¾—ç›¸é—œå­¸ç¾¤è³‡è¨Š
            top_groups = self.rag_system.retrieve_top_k_groups(request.query)
            context = self.generate_context(request.query, top_groups)

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            logger.info(f"å­¸ç¾¤æœå°‹å®Œæˆï¼Œè€—æ™‚ {duration} ç§’")

            return {
                "æŸ¥è©¢å…§å®¹": request.query,
                "æª¢ç´¢çµæœ": context,
                "æ¨è–¦å­¸ç¾¤": top_groups
            }
        except Exception as e:
            logger.error(f"å­¸ç¾¤æœå°‹ä½œæ¥­ç™¼ç”Ÿç•°å¸¸: {str(e)}")
            raise HTTPException(status_code=500, detail=f"å­¸ç¾¤æœå°‹ä½œæ¥­ç™¼ç”Ÿç•°å¸¸ï¼š{str(e)}")

    def generate_context(self, query: str, top_groups: List[Dict]) -> str:
        """
        ä½¿ç”¨ PromptTemplate ç”Ÿæˆæ›´æ™ºèƒ½çš„ä¸Šä¸‹æ–‡
        """
        # æ™ºèƒ½æ¨è–¦æç¤ºé¡å‹
        prompt_type = PromptTemplate.suggest_prompt_type(query)

        # ä½¿ç”¨å¢å¼·çš„ä¸Šä¸‹æ–‡ç”Ÿæˆæ–¹æ³•
        return PromptTemplate.generate_enhanced_context(query, top_groups, prompt_type)
